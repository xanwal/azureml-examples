{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and deploy a model with custom Docker Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, we will deploy inference servers on customized Docker images using Azure Secure Container Registry. We will extend a pre-built image from Azure's curated image library and build an image from base Ubuntu 18.04. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
    "\n",
    "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
    "\n",
    "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
    "\n",
    "* You must have an Azure Machine Learning workspace. \n",
    "\n",
    "* You must have an Azure Secure Container registry. One is created automatically created for a workspace without one upon first usage, however in this example we explicitly reference the container registry by name, so you need it beforehand. You can create one through the Azure Portal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first get a handle to the workspace, which will be reused later as we deploy images. You must already have an existing Azure Secure Container Registry associated with the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '<YOUR_SUBSCRIPTION_ID>'\n",
    "resource_group = '<YOUR_RESOURCE_GROUP>'\n",
    "workspace = '<YOUR_WORKSPACE>'\n",
    "container_registry_name = '<YOUR_CONTAINER_REGISTRY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '6fe1c377-b645-4e8e-b588-52e57cc856b2'\n",
    "resource_group = 'v-alwallace-test'\n",
    "workspace = 'valwallace'\n",
    "container_registry_name = 'valwallaceskr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from azure.ml import MLClient\n",
    "from azure.ml.entities import ManagedOnlineDeployment, ManagedOnlineEndpoint\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from random import randint\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Docker image deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define deployment and container registry details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the deployment, container registry, and container name are all required. We will create a new container using the name here, however, The endpoint name is optional, the code below will generate a random name likely to be unique within the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required\n",
    "deployment_name = 'docker-basic'\n",
    "container_name = 'docker-basic'\n",
    "# Optional\n",
    "endpoint_name = f'docker-basic-{randint(1e3,1e7)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image we will build is the OpenMPI3.1.2 Ubuntu 18.04 image from Azure. This image contains all of the dependencies required to score the model as well as an inference server. Our Dockerfile for this basic example is below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Dockerfile \n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will build the image locally and test a local deployment. If you're rebuilding, pass the `--no-cache` flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t {container_name} docker_basic/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now among your local images, which you can see by running the command  `docker image list` or `docker image ls`. The image is now ready to be included in a deployment, however, let's run the image now and see the AML Inference Server load. It comes preloaded in most of the Azure-curated images.  Since there are are no models and no scoring script provided to it yet, it will exit quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -t {container_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our container running, we will log in to Azure Container Registry to upload the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr login -n {container_registry_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPacking source code into tar to upload...\u001b[0m\n",
      "\u001b[93mUploading archived source code from '/tmp/build_archive_f0b9143ae2e24538afa73116a7075691.tar.gz'...\u001b[0m\n",
      "\u001b[93mSending context (2.056 KiB) to registry: valwallaceskr...\u001b[0m\n",
      "\u001b[K\u001b[93mQueued a build with ID: ch1f\u001b[0m\n",
      "\u001b[93mWaiting for an agent...\u001b[0m\n",
      "2022/04/20 21:30:43 Downloading source code...\n",
      "2022/04/20 21:30:44 Finished downloading source code\n",
      "2022/04/20 21:30:44 Using acb_vol_1f20f727-cde5-43a8-ad51-cd654e61c575 as the home volume\n",
      "2022/04/20 21:30:44 Setting up Docker configuration...\n",
      "2022/04/20 21:30:45 Successfully set up Docker configuration\n",
      "2022/04/20 21:30:45 Logging in to registry: valwallaceskr.azurecr.io\n",
      "2022/04/20 21:30:46 Successfully logged into valwallaceskr.azurecr.io\n",
      "2022/04/20 21:30:46 Executing step ID: build. Timeout(sec): 28800, Working directory: '', Network: ''\n",
      "2022/04/20 21:30:46 Scanning for dependencies...\n",
      "2022/04/20 21:30:47 Successfully scanned dependencies\n",
      "2022/04/20 21:30:47 Launching container with name: build\n",
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/1 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\n",
      "20210727.v1: Pulling from azureml/openmpi3.1.2-ubuntu18.04\n",
      "feac53061382: Pulling fs layer\n",
      "18ef48ef045d: Pulling fs layer\n",
      "5153f58622f0: Pulling fs layer\n",
      "22fbf22430c8: Pulling fs layer\n",
      "8dc32c00591d: Pulling fs layer\n",
      "2b3e4ca7948b: Pulling fs layer\n",
      "f1e8757b6afd: Pulling fs layer\n",
      "0d41d0383fc1: Pulling fs layer\n",
      "c074c1ef54d6: Pulling fs layer\n",
      "22fbf22430c8: Waiting\n",
      "8dc32c00591d: Waiting\n",
      "2b3e4ca7948b: Waiting\n",
      "f1e8757b6afd: Waiting\n",
      "0d41d0383fc1: Waiting\n",
      "c074c1ef54d6: Waiting\n",
      "feac53061382: Verifying Checksum\n",
      "feac53061382: Download complete\n",
      "5153f58622f0: Verifying Checksum\n",
      "5153f58622f0: Download complete\n",
      "22fbf22430c8: Verifying Checksum\n",
      "22fbf22430c8: Download complete\n",
      "2b3e4ca7948b: Verifying Checksum\n",
      "2b3e4ca7948b: Download complete\n",
      "18ef48ef045d: Verifying Checksum\n",
      "18ef48ef045d: Download complete\n",
      "0d41d0383fc1: Verifying Checksum\n",
      "0d41d0383fc1: Download complete\n",
      "8dc32c00591d: Verifying Checksum\n",
      "8dc32c00591d: Download complete\n",
      "c074c1ef54d6: Verifying Checksum\n",
      "c074c1ef54d6: Download complete\n",
      "feac53061382: Pull complete\n",
      "f1e8757b6afd: Verifying Checksum\n",
      "f1e8757b6afd: Download complete\n",
      "18ef48ef045d: Pull complete\n",
      "5153f58622f0: Pull complete\n",
      "22fbf22430c8: Pull complete\n",
      "8dc32c00591d: Pull complete\n",
      "2b3e4ca7948b: Pull complete\n",
      "f1e8757b6afd: Pull complete\n",
      "0d41d0383fc1: Pull complete\n",
      "c074c1ef54d6: Pull complete\n",
      "Digest: sha256:d326424673c56a738c4b46d815c6848f54f37046ab28a6ac66a2b97936ad5359\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\n",
      " ---> 9fab65be7722\n",
      "Successfully built 9fab65be7722\n",
      "Successfully tagged valwallaceskr.azurecr.io/docker-basic:latest\n",
      "2022/04/20 21:31:04 Successfully executed container: build\n",
      "2022/04/20 21:31:04 Executing step ID: push. Timeout(sec): 3600, Working directory: '', Network: ''\n",
      "2022/04/20 21:31:04 Pushing image: valwallaceskr.azurecr.io/docker-basic:latest, attempt 1\n",
      "The push refers to repository [valwallaceskr.azurecr.io/docker-basic]\n",
      "5d84bb803705: Preparing\n",
      "4db1734fd090: Preparing\n",
      "e761f728c5cd: Preparing\n",
      "cc1671468896: Preparing\n",
      "1b00e8d28c9f: Preparing\n",
      "49d342162f4f: Preparing\n",
      "eabfa7283fab: Preparing\n",
      "bf0b12f31e39: Preparing\n",
      "21639b09744f: Preparing\n",
      "49d342162f4f: Waiting\n",
      "eabfa7283fab: Waiting\n",
      "bf0b12f31e39: Waiting\n",
      "21639b09744f: Waiting\n",
      "e761f728c5cd: Layer already exists\n",
      "1b00e8d28c9f: Layer already exists\n",
      "cc1671468896: Layer already exists\n",
      "5d84bb803705: Layer already exists\n",
      "eabfa7283fab: Layer already exists\n",
      "4db1734fd090: Layer already exists\n",
      "49d342162f4f: Layer already exists\n",
      "bf0b12f31e39: Layer already exists\n",
      "21639b09744f: Layer already exists\n",
      "latest: digest: sha256:d326424673c56a738c4b46d815c6848f54f37046ab28a6ac66a2b97936ad5359 size: 2221\n",
      "2022/04/20 21:31:06 Successfully pushed image: valwallaceskr.azurecr.io/docker-basic:latest\n",
      "2022/04/20 21:31:06 Step ID: build marked as successful (elapsed time in seconds: 18.482697)\n",
      "2022/04/20 21:31:06 Populating digests for step ID: build...\n",
      "2022/04/20 21:31:07 Successfully populated digests for step ID: build\n",
      "2022/04/20 21:31:07 Step ID: push marked as successful (elapsed time in seconds: 1.746488)\n",
      "2022/04/20 21:31:07 The following dependencies were found:\n",
      "2022/04/20 21:31:07 \n",
      "- image:\n",
      "    registry: valwallaceskr.azurecr.io\n",
      "    repository: docker-basic\n",
      "    tag: latest\n",
      "    digest: sha256:d326424673c56a738c4b46d815c6848f54f37046ab28a6ac66a2b97936ad5359\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/openmpi3.1.2-ubuntu18.04\n",
      "    tag: 20210727.v1\n",
      "    digest: sha256:d326424673c56a738c4b46d815c6848f54f37046ab28a6ac66a2b97936ad5359\n",
      "  git: {}\n",
      "\n",
      "Run ID: ch1f was successful after 25s\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!az acr build --image {container_name} --registry {container_registry_name} {deployment_name}/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the inference server locally, we will proide the inference server with resources by setting our `Model`, `CodeConfiguration` and `Environment` in the ManagedOnlineDeployment YAML file. This file specifies the trained model `sklearn_regression_model.pkl1`, the scoring script under `score.py`, and the registry and repository of the image we built above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml \n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: deployment_name\n",
    "endpoint_name: endpoint_name\n",
    "model:\n",
    "  path: sklearn_regression_model.pkl\n",
    "code_configuration: \n",
    "  code: \".\"\n",
    "  scoring_script: score.py\n",
    "environment:\n",
    "  image: container_registry_name.azurecr.io/docker-basic:latest\n",
    "instance_type: Standard_F2s_v2\n",
    "instance_count: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the YAML file and update variables, however, the in your workloads the file can be directly loaded by passing the file path to the `.load` method of a `ManagedOnlineDeployment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docker-basic/deployment.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-25246fdaa875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'deployment.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdeployment_yaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docker-basic/deployment.yml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(os.path.join(deployment_name,'deployment.yml'),'r') as f:\n",
    "    deployment_yaml = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml['name'] = deployment_name\n",
    "deployment_yaml['endpoint_name'] = endpoint_name\n",
    "deployment_yaml['environment']['image'] = f'{container_registry_name}.azurecr.io/{container_name}:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy. First we create an endpoint and then a deployment. The code below shows two ways of configuring Azure Machine Learning entities using the Python SDK v2. We can provide configuration parameters either through arguments in the constructor, or through loading a YAML file. If you do not need to preprocess a YAML file, the `.load()` method enables you to pass a file path directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)\n",
    "deployment = ManagedOnlineDeployment.load_from_dict(deployment_yaml)\n",
    "deployment = ml_client.online_deployments.begin_create_or_update(deployment, local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below to see the deployment logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml online-deployment get-logs -n docker-basic -e {endpoint_name} --local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the local endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test an endpoint, we need the scoring URI and an authentication key. When we called `.begin_create_or_update` above, the ml_client returned the endpoint object to us with metadata about the deployment, including the attribute `scoring_uri`. If we didn't have a reference to the endpoint, we would call `ml_client.online_endpoints.get(name=<ENDPOINT_NAME>)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = ml_client.online_endpoints.list_keys(endpoint_name).primary_key\n",
    "endpoint = ml_client.online_endpoints.get(endpoint_name,local=True)\n",
    "scoring_uri = endpoint.scoring_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online endpoints' scoring URIs end with `/score`. To check the aliveness of the endpoint without scoring data, a GET request can be made to the base URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(scoring_uri[:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score data using REST, insert the auth token in the header, load the sample JSON file, and make a POST request to the scoring URI, which ends with `/score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('sample-request.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "headers = {'Authorization' : f'Bearer {auth_token}'} \n",
    "response = requests.post(url=scoring_uri,\n",
    "                        headers=headers,\n",
    "                        data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Deployment\n",
    "The scoring server can be deployed in the cloud with few configuration changes. Our Docker image is already built and available in the Azure Container Registry, and our deployment YAML file requires no changes. We first generate a new online endpoint name and proceed with similar steps as above. Note the removal of the `local=True` argument in `ml_client` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'docker-basic-{randint(1e3,1e7)}'\n",
    "\n",
    "import yaml\n",
    "with open('deployment_local.yml','r') as f:\n",
    "    deployment_yaml = yaml.safe_load(f)\n",
    "\n",
    "deployment_yaml['endpoint_name'] = endpoint_name\n",
    "deployment_yaml['environment']['image'] = f'{container_registry_name}.azurecr.io/{container_name}:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "deployment = ManagedOnlineDeployment.load_from_dict(deployment_yaml)\n",
    "deployment = ml_client.online_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will score the model using the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample-request.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "ml_client.online_endpoints.invoke(endpoint.name,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preinstall a requirements.txt file using the AML Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will extend the  no-framework inference Docker image from [Azure's curated image library](/azure/machine-learning/concept-prebuilt-docker-images-inference). This image is built from a minimal Ubuntu 18.04 base image and does not include any frameworks such as Tensorflow or Torch, however, it does include the Azure Machine Learning Inference Server, which enables the rapid deployment of inference servers through a single `score.py` file that calls the scored model. Our working directory looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these files will be copied into the image in the Dockerfile. The model directory contains the trained model object we will call to score each request. This path will be passed to the Inferencing Server, and may contain nested subdirectory trees corresponding to different models and verisons. The `score.py` file is located in the code directory. The inferencing server will call the score.py file from the relevant subdirectory depending on the model version, so there is no need for the score.py file to keep track of this tree. The requirements.txt file contains the additional Python packages we will install in the image. It looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "numpy==1.21.2\n",
    "pip==21.2.4\n",
    "scikit-learn==0.24.2\n",
    "scipy==1.7.1\n",
    "azureml-defaults==1.38.0\n",
    "inference-schema[numpy-support]==1.3.0\n",
    "joblib==1.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this deployment, we will  After image creation, requirements files can be dynamically loaded by the inferencing server or additional dependencies can be specified through an `Environment`. See the Environment and ManagedOnlineDeployment schemas for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
    "USER root:root\n",
    "COPY environment /var/environment\n",
    "RUN pip install -r /var/environment/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr login --name {container_registry_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr build --image custom_container --registry {container_registry_name} --file Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a managed online deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we deploy an online endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required\n",
    "deployment_name = 'docker-ouo'\n",
    "container_name = 'docker-pip'\n",
    "# Optional\n",
    "endpoint_name = f'docker-pip-{randint(1e3,1e7)}'\n",
    "ml_client.online_endpoints.begin_create_or_update(ManagedOnlineEndpoint(name=endpoint_name))\n",
    "deployment = ManagedOnlineDeployment.load(os.path.join(deployment_name,'deployment.yml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = ml_client.online_endpoints.list_keys(endpoint_name).primary_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = ml_client.online_endpoints.list_keys(endpoint_name).primary_key\n",
    "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
    "scoring_uri = endpoint.scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=None\n",
    "import requests\n",
    "import json \n",
    "with open(os.path.join('.','sample-request.json')) as f:\n",
    "    data = json.loads(f.read())\n",
    "headers = {}\n",
    "headers = {'Authorization' : f'Bearer {auth_token}', 'Content-Type':'application/json'} \n",
    "#scoring_uri = \"https://custom-container-9230.eastus2.inference.ml.azure.com/score\"\n",
    "response = requests.post(url=scoring_uri,\n",
    "                        headers=headers,\n",
    "                        data=json.dumps(data))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50db1dce3900200bed9bfd600df88d2ae354b85acd9708fa0a0be5ff9bc29e40"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('azureml-examples')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
