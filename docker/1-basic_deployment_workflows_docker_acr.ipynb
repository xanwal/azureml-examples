{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic managed deployments with Docker and ACR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we will deploy an inference server using a minimal custom Docker image and Azure Secure Container Registry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
    "\n",
    "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
    "\n",
    "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
    "\n",
    "* You must have an Azure Machine Learning workspace. \n",
    "\n",
    "* You must have an Azure Secure Container registry. One is created automatically created for a workspace without one upon first usage, however in this example we explicitly reference the container registry by name, so you need it beforehand. You can create one through the Azure Portal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first get a handle to the workspace, which will be reused later as we deploy images. You must already have an existing Azure Secure Container Registry associated with the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '<YOUR_SUBSCRIPTION_ID>'\n",
    "resource_group = '<YOUR_RESOURCE_GROUP>'\n",
    "workspace = '<YOUR_WORKSPACE>'\n",
    "container_registry_name = '<YOUR_CONTAINER_REGISTRY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ml import MLClient\n",
    "from azure.containerregistry import ContainerRegistryClient\n",
    "from azure.ml.entities import ManagedOnlineDeployment, ManagedOnlineEndpoint\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from random import randint\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Docker workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define deployment and container registry details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the deployment, container registry, and container name are all required. We will create a new container using the name here, however, The endpoint name is optional, the code below will generate a random name likely to be unique within the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "# Required\n",
    "deployment_name = 'docker-basic'\n",
    "container_name = 'docker-basic'\n",
    "# Optional\n",
    "endpoint_name = f'docker-basic-{randint(1e3,1e7)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image we will build is the Sklearn-0.24 Ubuntu 18.04 image from Azure. This image contains all of the dependencies required to score the model as well as an inference server. Our Dockerfile for this basic example is below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Dockerfile \n",
    "FROM mcr.microsoft.com/azureml/sklearn-0.24.1-ubuntu18.04-py37-cpu-inference:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will build the image and test a local deployment. If you're rebuilding, pass the `--no-cache` flag. We can build and test the image using Docker itself, however, with no scorin script or trained models, the container will fail immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log in to ACR  and build an image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login\n",
    "!az acr login -n {container_registry_name}\n",
    "!docker build {container_name} docker_basic/. \n",
    "!docker image ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the locally-trained image to ACR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login {container_registry_name}.azurecr.io\n",
    "!docker tag {container_name} {container_registry_name}.azurecr.io/storage-client\n",
    "!docker push {container_registry_name}.azurecr.io/storage-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build directly with the ACR CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr login -n {container_registry_name}\n",
    "!az acr build --image {container_name} --registry {container_registry_name}  ./{deployment_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the inference server locally, we will proide the inference server with resources by setting our `Model`, `CodeConfiguration` and `Environment` in the ManagedOnlineDeployment YAML file. This file specifies the trained model `sklearn_regression_model.pkl1`, the scoring script under `score.py`, and the registry and repository of the image we built above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml \n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: deployment_name\n",
    "endpoint_name: endpoint_name\n",
    "model:\n",
    "  path: sklearn_regression_model.pkl\n",
    "code_configuration: \n",
    "  code: \".\"\n",
    "  scoring_script: score.py\n",
    "environment:\n",
    "  image: container_registry_name.azurecr.io/docker-basic:latest\n",
    "instance_type: Standard_F2s_v2\n",
    "instance_count: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an online endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import deployment YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the YAML file for the deployment and update variables, however, the in your workloads the file can be directly loaded by passing the file path to the `.load` method of a `ManagedOnlineDeployment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(f'{deployment_name}/deployment.yml','r') as f:\n",
    "    deployment_yaml = yaml.safe_load(f)\n",
    "deployment_yaml['name'] = deployment_name\n",
    "deployment_yaml['endpoint_name'] = endpoint_name\n",
    "deployment_yaml['environment']['image'] = f'{container_registry_name}.azurecr.io/{container_name}:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy. First we create an endpoint and then a deployment. The code below shows two ways of configuring Azure Machine Learning entities using the Python SDK v2. We can provide configuration parameters either through arguments in the constructor, or through loading a YAML file. If you do not need to preprocess a YAML file, the `.load()` method enables you to pass a file path directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ManagedOnlineDeployment.load_from_dict(deployment_yaml)\n",
    "deployment = ml_client.online_deployments.begin_create_or_update(deployment, local=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check deployment logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml online-deployment get-logs -n docker-basic -e {endpoint_name} --local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the local endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get token and scoring URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test an endpoint, we need the scoring URI and an authentication key. When we called `.begin_create_or_update` above, the ml_client returned the endpoint object to us with metadata about the deployment, including the attribute `scoring_uri`. If we didn't have a reference to the endpoint, we would call `ml_client.online_endpoints.get(name=<ENDPOINT_NAME>)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = ml_client.online_endpoints.list_keys(endpoint_name).primary_key\n",
    "endpoint = ml_client.online_endpoints.get(endpoint_name,local=True)\n",
    "scoring_uri = endpoint.scoring_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score with REST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online endpoints' scoring URIs end with `/score`. To check the aliveness of the endpoint without scoring data, a GET request can be made to the base URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(scoring_uri[:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score data using REST, insert the auth token in the header, load the sample JSON file, and make a POST request to the scoring URI, which ends with `/score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('sample-request.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "headers = {'Authorization' : f'Bearer {auth_token}'} \n",
    "response = requests.post(url=scoring_uri,\n",
    "                        headers=headers,\n",
    "                        data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to the Cloud\n",
    "The scoring server can be deployed in the cloud with few configuration changes. Our Docker image is already built and available in the Azure Container Registry, and our deployment YAML file requires no changes. We first generate a new online endpoint name and proceed with similar steps as above. Note the removal of the `local=True` argument in `ml_client` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a new endpoint name and import YAML\n",
    "There is no change to our YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'docker-basic-{randint(1e3,1e7)}'\n",
    "\n",
    "import yaml\n",
    "with open(os.path.join(deployment_name, 'deployment.yml'),'r') as f:\n",
    "    deployment_yaml = yaml.safe_load(f)\n",
    "\n",
    "deployment_yaml['endpoint_name'] = endpoint_name\n",
    "deployment_yaml['environment']['image'] = f'{container_registry_name}.azurecr.io/{container_name}:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the endpoint and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "deployment = ManagedOnlineDeployment.load_from_dict(deployment_yaml)\n",
    "adeployment = ml_client.online_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and score the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will score the model using the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath\n",
    "json_file_path = PurePath(os.path.join(deployment_name, 'sample-request.json'))\n",
    "ml_client.online_endpoints.invoke(endpoint_name=endpoint_name, deployment_name=deployment_name, request_file=json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML Files vs AML Python Objects and Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure YAML files are powerful, concise tools. The Python interface to Azure Machine Learning offers a complementary interface. The AML Python SDK v2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-loading dependencies in Docker images\n",
    "Azure's flexible environment specifications make it trivially easy to deploy with pip, conda, or both, as well as custom code configurations. However in many circumstances it is necessary or advantageous preload as much as possible. Pushing dependencies to bulid time can help reduce deployment time or the size of the codebase, a prime use case for Docker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
    "\n",
    "RUN apt install python3, python3-pip\n",
    "COPY requirements.txt /tmp/requirements.txt\n",
    "RUN pip install -r /tmp/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50db1dce3900200bed9bfd600df88d2ae354b85acd9708fa0a0be5ff9bc29e40"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('azureml-examples')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
