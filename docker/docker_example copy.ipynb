{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and deploy a model with custom Docker Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will deploy inference servers on customized Docker images using Azure Secure Container Registry. We will extend a pre-built image from Azure's curated image library and build an image from base Ubuntu 18.04. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
    "\n",
    "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
    "\n",
    "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
    "\n",
    "* You must have an Azure Machine Learning workspace. \n",
    "\n",
    "* You must have an Azure Secure Container registry. One is created automatically created for a workspace without one upon first usage, however in this example we explicitly reference the container registry by name, so you need it beforehand. You can create one through the Azure Portal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first get a handle to the workspace, which will be reused later as we deploy images. You must already have an existing Azure Secure Container Registry associated with the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '6fe1c377-b645-4e8e-b588-52e57cc856b2'\n",
    "resource_group = 'v-alwallace-test'\n",
    "workspace = 'valwallace'\n",
    "container_registry_name = 'valwallaceskr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.ml.entities import ManagedOnlineDeployment, ManagedOnlineEndpoint\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from random import randint\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Docker image deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define deployment and container registry details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the deployment, container registry, and container name are all required. We will create a new container using the name here, however, The endpoint name is optional, the code below will generate a random name likely to be unique within the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required\n",
    "deployment_name = 'docker-basic'\n",
    "container_name = 'docker-basic'\n",
    "# Optional\n",
    "endpoint_name = f'docker-basic-{randint(1e3,1e7)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image we will build is the OpenMPI3.1.2 Ubuntu 18.04 image from Azure. This image contains all of the dependencies required to score the model, as well as the AML Inference Server, so our Dockerfile is trivial: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Dockerfile \n",
    "FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will build the image locally and test a local deployment. If you're rebuilding, pass the `--no-cache` flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (5/5) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for mcr.microsoft.com/azureml/minimal-ubuntu  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [1/1] FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:559246eab2756080b3b4f164b3a20dc9da5e7bc9ed709  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/docker-basic                            0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -t {container_name} docker_basic/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now among your local images, which you can see by running the command  `docker image list` or `docker image ls`. The image is now ready to be included in a deployment, however, let's run the image now and see the AML Inference Server load. It comes preloaded in most of the Azure-curated images.  Since there are are no models and no scoring script provided to it yet, it will exit quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-20T17:39:38,346349041+00:00 - rsyslog/run \n",
      "2022-04-20T17:39:38,346439343+00:00 - gunicorn/run \n",
      "2022-04-20T17:39:38,347756537+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:38,349158728+00:00 | gunicorn/run | ###############################################\n",
      "2022-04-20T17:39:38,350396320+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2022-04-20T17:39:38,351563140+00:00 | gunicorn/run | ###############################################\n",
      "2022-04-20T17:39:38,353014974+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:38,353315738+00:00 - nginx/run \n",
      "2022-04-20T17:39:38,357228326+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:38,360816232+00:00 | gunicorn/run | AzureML image information: minimal-ubuntu18.04-py37-cpu-inference:20220419.v1\n",
      "2022-04-20T17:39:38,362060015+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:38,363385238+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:38,364850028+00:00 | gunicorn/run | PATH environment variable: /opt/miniconda/envs/amlenv/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2022-04-20T17:39:38,366332335+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2022-04-20T17:39:38,367711079+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:38,369601423+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "nginx: [warn] the \"user\" directive makes sense only if the master process runs with super-user privileges, ignored in /etc/nginx/nginx.conf:1\n",
      "azure-core==1.23.1\n",
      "azure-identity==1.9.0\n",
      "azureml-inference-server-http==0.6.0\n",
      "cachetools==5.0.0\n",
      "certifi==2021.10.8\n",
      "cffi==1.15.0\n",
      "charset-normalizer==2.0.12\n",
      "click==7.1.2\n",
      "cryptography==36.0.2\n",
      "Flask==1.0.3\n",
      "google-api-core==2.7.2\n",
      "google-auth==2.6.5\n",
      "googleapis-common-protos==1.56.0\n",
      "gunicorn==20.1.0\n",
      "idna==3.3\n",
      "inference-schema==1.3.2\n",
      "itsdangerous==1.1.0\n",
      "Jinja2==3.0.3\n",
      "MarkupSafe==2.1.1\n",
      "msal==1.17.0\n",
      "msal-extensions==0.3.1\n",
      "opencensus==0.8.0\n",
      "opencensus-context==0.1.2\n",
      "opencensus-ext-azure==1.1.3\n",
      "portalocker==2.4.0\n",
      "protobuf==3.20.0\n",
      "psutil==5.9.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "PyJWT==2.3.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.1\n",
      "requests==2.27.1\n",
      "rsa==4.8\n",
      "six==1.16.0\n",
      "typing-extensions==4.2.0\n",
      "urllib3==1.26.9\n",
      "Werkzeug==1.0.1\n",
      "wrapt==1.12.1\n",
      "\n",
      "2022-04-20T17:39:40,516212581+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:40,517982840+00:00 | gunicorn/run | Entry script directory: /var/azureml-app\n",
      "2022-04-20T17:39:40,519621142+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:40,521751742+00:00 | gunicorn/run | ###############################################\n",
      "2022-04-20T17:39:40,523412435+00:00 | gunicorn/run | Dynamic Python Package Installation\n",
      "2022-04-20T17:39:40,525180334+00:00 | gunicorn/run | ###############################################\n",
      "2022-04-20T17:39:40,526556726+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:40,528150860+00:00 | gunicorn/run | Dynamic Python package installation is disabled.\n",
      "2022-04-20T17:39:40,529661764+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:40,531133185+00:00 | gunicorn/run | ###############################################\n",
      "2022-04-20T17:39:40,532699813+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2022-04-20T17:39:40,534144064+00:00 | gunicorn/run | ###############################################\n",
      "2022-04-20T17:39:40,535448434+00:00 | gunicorn/run | \n",
      "2022-04-20T17:39:40,538195558+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "The environment variable 'AZUREML_MODEL_DIR' has not been set.\n",
      "Use the --model_dir command line argument to set it.\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.6.0\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: main.py\n",
      "Model Directory: None\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.6.0\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://0.0.0.0:31311 (10)\n",
      "Using worker: sync\n",
      "Booting worker with pid: 58\n",
      "Exception in worker process\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/arbiter.py\", line 589, in spawn_worker\n",
      "    worker.init_process()\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/workers/base.py\", line 134, in init_process\n",
      "    self.load_wsgi()\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/workers/base.py\", line 146, in load_wsgi\n",
      "    self.wsgi = self.app.wsgi()\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/app/base.py\", line 67, in wsgi\n",
      "    self.callable = self.load()\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py\", line 58, in load\n",
      "    return self.load_wsgiapp()\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py\", line 48, in load_wsgiapp\n",
      "    return util.import_app(self.app_uri)\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/gunicorn/util.py\", line 359, in import_app\n",
      "    mod = importlib.import_module(module)\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/azureml_inference_server_http/server/entry.py\", line 1, in <module>\n",
      "    import create_app\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/azureml_inference_server_http/server/create_app.py\", line 24, in <module>\n",
      "    from routes import main\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/azureml_inference_server_http/server/routes.py\", line 39, in <module>\n",
      "    from aml_blueprint import AMLBlueprint\n",
      "  File \"/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/azureml_inference_server_http/server/aml_blueprint.py\", line 33, in <module>\n",
      "    main_module_spec.loader.exec_module(main)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 859, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_data\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/azureml-app/main.py'\n",
      "Worker exiting (pid: 58)\n",
      "Shutting down: Master\n",
      "Reason: Worker failed to boot.\n",
      "2022-04-20T17:39:41,705867243+00:00 - gunicorn/finish 3 0\n",
      "2022-04-20T17:39:41,707039847+00:00 - Exit code 3 is not normal. Killing image.\n"
     ]
    }
   ],
   "source": [
    "!docker run -t {container_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: acr: command not found\n"
     ]
    }
   ],
   "source": [
    "!acr build image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix that by including the image in an environment through an YAML file, along with a trained model and `score.py` script for the Inference Server to call. This `deployment.yml` file specifies the trained model file under `model` as well as the scoring script under `code_configuration`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml \n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: deployment_name\n",
    "endpoint_name: endpoint_name\n",
    "model:\n",
    "  path: './docker_basic/sklearn_regression_model.pkl'\n",
    "code_configuration: \n",
    "  code: './docker_basic'\n",
    "  scoring_script: 'score.py'\n",
    "environment:\n",
    "  image: container_name:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the YAML file and update variables, however, the in your workloads the file can be directly loaded by passing the file path to the `.load` method of a `ManagedOnlineDeployment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('./docker_basic/deployment_local.yml','r') as f:\n",
    "    deployment_yaml = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml['name'] = deployment_name\n",
    "deployment_yaml['endpoint_name'] = endpoint_name\n",
    "deployment_yaml['environment']['image'] = f'{container_name}:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('$schema',\n",
       "              'https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json'),\n",
       "             ('name', 'docker-basic'),\n",
       "             ('endpoint_name', 'docker-basic-9455673'),\n",
       "             ('model',\n",
       "              OrderedDict([('path',\n",
       "                            './docker_basic/sklearn_regression_model.pkl')])),\n",
       "             ('code_configuration',\n",
       "              OrderedDict([('code', './docker_basic'),\n",
       "                           ('scoring_script', 'score.py')])),\n",
       "             ('environment', OrderedDict([('image', 'docker-basic:latest')])),\n",
       "             ('instance_type', 'Standard_F2s_v2'),\n",
       "             ('instance_count', 1)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy. First we create an endpoint and then a deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPacking source code into tar to upload...\u001b[0m\n",
      "\u001b[93mUploading archived source code from '/tmp/build_archive_7f023d29e8b74ab78759fd7d4f0730a4.tar.gz'...\u001b[0m\n",
      "\u001b[93mSending context (2.171 KiB) to registry: valwallaceskr...\u001b[0m\n",
      "\u001b[K\u001b[93mQueued a build with ID: cht\u001b[0m\n",
      "\u001b[93mWaiting for an agent...\u001b[0m\n",
      "2022/04/20 17:49:57 Downloading source code...\n",
      "2022/04/20 17:49:59 Finished downloading source code\n",
      "2022/04/20 17:49:59 Using acb_vol_4394860d-6489-4e3a-a60a-155354bb4c1d as the home volume\n",
      "2022/04/20 17:49:59 Setting up Docker configuration...\n",
      "2022/04/20 17:50:00 Successfully set up Docker configuration\n",
      "2022/04/20 17:50:00 Logging in to registry: valwallaceskr.azurecr.io\n",
      "2022/04/20 17:50:01 Successfully logged into valwallaceskr.azurecr.io\n",
      "2022/04/20 17:50:01 Executing step ID: build. Timeout(sec): 28800, Working directory: '', Network: ''\n",
      "2022/04/20 17:50:01 Scanning for dependencies...\n",
      "2022/04/20 17:50:01 Successfully scanned dependencies\n",
      "2022/04/20 17:50:01 Launching container with name: build\n",
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/7 : FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
      "latest: Pulling from azureml/minimal-ubuntu18.04-py37-cpu-inference\n",
      "08a6abff8943: Already exists\n",
      "3e3b10a6df77: Pulling fs layer\n",
      "e2679f94c58d: Pulling fs layer\n",
      "4fc395183073: Pulling fs layer\n",
      "9c764d6bea2e: Pulling fs layer\n",
      "427ec8e1aeb4: Pulling fs layer\n",
      "1a2c88aaafd5: Pulling fs layer\n",
      "ff47b1136350: Pulling fs layer\n",
      "b175d4f516ce: Pulling fs layer\n",
      "14476166ea04: Pulling fs layer\n",
      "b95ef39676ef: Pulling fs layer\n",
      "947d0cc211a8: Pulling fs layer\n",
      "b3629edb2043: Pulling fs layer\n",
      "c4aace54b1e9: Pulling fs layer\n",
      "9c764d6bea2e: Waiting\n",
      "427ec8e1aeb4: Waiting\n",
      "1a2c88aaafd5: Waiting\n",
      "ff47b1136350: Waiting\n",
      "b175d4f516ce: Waiting\n",
      "14476166ea04: Waiting\n",
      "b95ef39676ef: Waiting\n",
      "947d0cc211a8: Waiting\n",
      "b3629edb2043: Waiting\n",
      "c4aace54b1e9: Waiting\n",
      "4fc395183073: Verifying Checksum\n",
      "4fc395183073: Download complete\n",
      "e2679f94c58d: Verifying Checksum\n",
      "e2679f94c58d: Download complete\n",
      "9c764d6bea2e: Verifying Checksum\n",
      "9c764d6bea2e: Download complete\n",
      "3e3b10a6df77: Verifying Checksum\n",
      "3e3b10a6df77: Download complete\n",
      "427ec8e1aeb4: Verifying Checksum\n",
      "427ec8e1aeb4: Download complete\n",
      "1a2c88aaafd5: Verifying Checksum\n",
      "1a2c88aaafd5: Download complete\n",
      "14476166ea04: Verifying Checksum\n",
      "14476166ea04: Download complete\n",
      "b175d4f516ce: Verifying Checksum\n",
      "b175d4f516ce: Download complete\n",
      "947d0cc211a8: Verifying Checksum\n",
      "947d0cc211a8: Download complete\n",
      "b3629edb2043: Verifying Checksum\n",
      "b3629edb2043: Download complete\n",
      "c4aace54b1e9: Verifying Checksum\n",
      "c4aace54b1e9: Download complete\n",
      "ff47b1136350: Verifying Checksum\n",
      "ff47b1136350: Download complete\n",
      "b95ef39676ef: Verifying Checksum\n",
      "b95ef39676ef: Download complete\n",
      "3e3b10a6df77: Pull complete\n",
      "e2679f94c58d: Pull complete\n",
      "4fc395183073: Pull complete\n",
      "9c764d6bea2e: Pull complete\n",
      "427ec8e1aeb4: Pull complete\n",
      "1a2c88aaafd5: Pull complete\n",
      "ff47b1136350: Pull complete\n",
      "b175d4f516ce: Pull complete\n",
      "14476166ea04: Pull complete\n",
      "b95ef39676ef: Pull complete\n",
      "947d0cc211a8: Pull complete\n",
      "b3629edb2043: Pull complete\n",
      "c4aace54b1e9: Pull complete\n",
      "Digest: sha256:f3aceb677a588d571c1bbef4ecab7d1ccb5001fd54d658350b618975a011529e\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
      " ---> 5a5e4d9e6b11\n",
      "Step 2/7 : COPY environment /var/environment\n",
      "COPY failed: file not found in build context or excluded by .dockerignore: stat environment: file does not exist\n",
      "2022/04/20 17:50:13 Container failed during run: build. No retries remaining.\n",
      "failed to run step ID: build: exit status 1\n",
      "\n",
      "Run ID: cht failed after 16s. Error: failed during run, err: exit status 1\n",
      "\u001b[91mRun failed\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!docker run -d -p 5000:5000 --restart=always --name registry registry:latest\n",
    "#!docker tag localdomain:5000/docker-basic  docker-basic\n",
    "# az acr login\n",
    "!az acr build --image {container_name} --registry {container_registry_name} --file Dockerfile docker_basic/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml['environment']['image'] = f'localhost:5000/{container_name}:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('name', 'docker-basic'),\n",
       "             ('endpoint_name', 'docker-basic-9455673'),\n",
       "             ('model',\n",
       "              OrderedDict([('path',\n",
       "                            './docker_basic/sklearn_regression_model.pkl')])),\n",
       "             ('code_configuration',\n",
       "              OrderedDict([('code', './docker_basic'),\n",
       "                           ('scoring_script', 'score.py')])),\n",
       "             ('environment',\n",
       "              OrderedDict([('image',\n",
       "                            'localhost:5000/myadmin/docker-basic:latest')])),\n",
       "             ('instance_type', 'Standard_F2s_v2'),\n",
       "             ('instance_count', 1)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating local endpoint (docker-basic-9455673) .Done (0m 5s)\n",
      "Creating local deployment (docker-basic-9455673 / docker-basic) .\n",
      "Building Docker image from Dockerfile.......................\n",
      "Step 1/5 : FROM docker-basic:latest\n",
      "pull access denied for docker-basic, repository does not exist or may require 'docker login': denied: requested access to the resource is deniedDone (2m 0s)\n"
     ]
    },
    {
     "ename": "LocalEndpointImageBuildError",
     "evalue": "Building the local endpoint image failed with error: pull access denied for docker-basic, repository does not exist or may require 'docker login': denied: requested access to the resource is denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLocalEndpointImageBuildError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0e490c7bc4d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mml_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_endpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_create_or_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeployment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mManagedOnlineDeployment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_yaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdeployment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_deployments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_create_or_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_telemetry/activity.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlog_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_operations/online_deployment_operations.py\u001b[0m in \u001b[0;36mbegin_create_or_update\u001b[0;34m(self, deployment, local, vscode_debug, no_wait)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             return self._local_deployment_helper.create_or_update(\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mdeployment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_endpoint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_local_endpoint_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvscode_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_operations/_local_deployment_helper.py\u001b[0m in \u001b[0;36mcreate_or_update\u001b[0;34m(self, deployment, local_endpoint_mode)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mlocal_endpoint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_endpoint_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mendpoint_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdeployment_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_utils/_endpoint_utils.py\u001b[0m in \u001b[0;36mlocal_endpoint_polling_wrapper\u001b[0;34m(func, message, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mpolling_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_operations/_local_deployment_helper.py\u001b[0m in \u001b[0;36m_create_deployment\u001b[0;34m(self, endpoint_name, deployment, local_endpoint_mode, endpoint_metadata, deployment_metadata)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mazureml_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLocalEndpointConstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOCKER_PORT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mlocal_endpoint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_endpoint_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_local_endpoints/docker_client.py\u001b[0m in \u001b[0;36mcreate_deployment\u001b[0;34m(self, endpoint_name, deployment_name, endpoint_metadata, deployment_metadata, build_directory, dockerfile_path, conda_source_path, conda_yaml_contents, volumes, environment, azureml_port, local_endpoint_mode)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mdockerfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdockerfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mconda_source_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconda_source_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mconda_yaml_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconda_yaml_contents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml-examples/lib/python3.7/site-packages/azure/ml/_local_endpoints/docker_client.py\u001b[0m in \u001b[0;36m_build_image\u001b[0;34m(self, build_directory, image_name, dockerfile_path, conda_source_path, conda_yaml_contents)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0mmodule_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mLocalEndpointImageBuildError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLocalEndpointImageBuildError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLocalEndpointImageBuildError\u001b[0m: Building the local endpoint image failed with error: pull access denied for docker-basic, repository does not exist or may require 'docker login': denied: requested access to the resource is denied"
     ]
    }
   ],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)\n",
    "deployment = ManagedOnlineDeployment.load_from_dict(deployment_yaml)\n",
    "deployment = ml_client.online_deployments.begin_create_or_update(deployment, local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend a curated Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will extend the  no-framework inference Docker image from [Azure's curated image library](/azure/machine-learning/concept-prebuilt-docker-images-inference). This image is built from a minimal Ubuntu 18.04 base image and does not include any frameworks such as Tensorflow or Torch, however, it does include the Azure Machine Learning Inference Server, which enables the rapid deployment of inference servers through a single `score.py` file that calls the scored model. Our working directory looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "model/\n",
    "    sklearn_regression_model.pkl\n",
    "environment/\n",
    "    requirements.txt\n",
    "code/\n",
    "    score.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these directories will be copied into the image in the Dockerfile. The model directory contains the trained model object we will call to score each request. This path will be passed to the Inferencing Server, and may contain nested subdirectory trees corresponding to different models and verisons. The `score.py` file is located in the code directory. The inferencing server will call the score.py file from the relevant subdirectory depending on the model version, so there is no need for the score.py file to keep track of this tree. The requirements.txt file contains the additional Python packages we will install in the image. It looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "numpy==1.21.2\n",
    "pip==21.2.4\n",
    "scikit-learn==0.24.2\n",
    "scipy==1.7.1\n",
    "azureml-defaults==1.38.0\n",
    "inference-schema[numpy-support]==1.3.0\n",
    "joblib==1.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this basic deployment, we will be installing these packages in the default Python path, which is configured ahead of time to support the inferencing server. More robust configurations using virtualenvs or `conda` environments are possible. After image creation, requirements files can be dynamically loaded by the inferencing server or additional dependencies can be specified through an `Environment`. See the Environment and ManagedOnlineDeployment schemas for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
    "USER root:root\n",
    "COPY environment /var/environment\n",
    "RUN pip install -r /var/environment/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we copy the model and code directories into the image, and make `ENV` variables to specify the scoring script and model directory. The Inferencing Server determines the correct entrypoints using these variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER dockeruser\n",
    "\n",
    "COPY code /var/azureml-app\n",
    "ENV AZUREML_ENTRY_SCRIPT=score.py\n",
    "\n",
    "COPY model /var/azureml-app/azureml-models\n",
    "ENV AZUREML_MODEL_DIR=/var/azureml-app/azureml-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the base image is referenced, we configure the additional packages and libraries required to score the model. For this base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend an Azure curated Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by extending the frameworkless Azure image built on minimal Ubuntu 18.04. While it is a minimal build, it does come with the Azure Inferencing Server, so scored models can be easily integrated into the image by providing the usual `score.py` entrypoint. With a pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr login --name {container_registry_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr build --image custom_container --registry {container_registry_name} --file Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a managed online deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we deploy an online endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(ManagedOnlineEndpoint(name='custom-container-9230'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ManagedOnlineDeployment.load('deployment.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = ml_client.online_endpoints.list_keys(endpoint_name).primary_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
    "scoring_uri = endpoint.scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response=None\n",
    "import requests\n",
    "import json \n",
    "with open(os.path.join('.','sample-request.json')) as f:\n",
    "    data = json.loads(f.read())\n",
    "headers = {}\n",
    "headers = {'Authorization' : f'Bearer {auth_token}', 'Content-Type':'application/json'} \n",
    "#scoring_uri = \"https://custom-container-9230.eastus2.inference.ml.azure.com/score\"\n",
    "response = requests.post(url=scoring_uri,\n",
    "                        headers=headers,\n",
    "                        data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend a curated Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will extend the  no-framework inference Docker image from [Azure's curated image library](/azure/machine-learning/concept-prebuilt-docker-images-inference). This image is built from a minimal Ubuntu 18.04 base image and does not include any frameworks such as Tensorflow or Torch, however, it does include the Azure Machine Learning Inference Server, which enables the rapid deployment of inference servers through a single `score.py` file that calls the scored model. Our working directory looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "model/\n",
    "    sklearn_regression_model.pkl\n",
    "environment/\n",
    "    requirements.txt\n",
    "code/\n",
    "    score.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these directories will be copied into the image in the Dockerfile. The model directory contains the trained model object we will call to score each request. This path will be passed to the Inferencing Server, and may contain nested subdirectory trees corresponding to different models and verisons. The `score.py` file is located in the code directory. The inferencing server will call the score.py file from the relevant subdirectory depending on the model version, so there is no need for the score.py file to keep track of this tree. The requirements.txt file contains the additional Python packages we will install in the image. It looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "numpy==1.21.2\n",
    "pip==21.2.4\n",
    "scikit-learn==0.24.2\n",
    "scipy==1.7.1\n",
    "azureml-defaults==1.38.0\n",
    "inference-schema[numpy-support]==1.3.0\n",
    "joblib==1.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this basic deployment, we will be installing these packages in the default Python path, which is configured ahead of time to support the inferencing server. More robust configurations using virtualenvs or `conda` environments are possible. After image creation, requirements files can be dynamically loaded by the inferencing server or additional dependencies can be specified through an `Environment`. See the Environment and ManagedOnlineDeployment schemas for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n",
    "USER root:root\n",
    "COPY environment /var/environment\n",
    "RUN pip install -r /var/environment/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we copy the model and code directories into the image, and make `ENV` variables to specify the scoring script and model directory. The Inferencing Server determines the correct entrypoints using these variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER dockeruser\n",
    "\n",
    "COPY code /var/azureml-app\n",
    "ENV AZUREML_ENTRY_SCRIPT=score.py\n",
    "\n",
    "COPY model /var/azureml-app/azureml-models\n",
    "ENV AZUREML_MODEL_DIR=/var/azureml-app/azureml-models"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50db1dce3900200bed9bfd600df88d2ae354b85acd9708fa0a0be5ff9bc29e40"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('azureml-examples')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
