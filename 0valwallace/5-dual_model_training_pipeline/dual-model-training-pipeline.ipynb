{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train two image classification models in one pipeline\n",
    "In this sample,  we train two simple CNN image classification models. \n",
    "\n",
    "First, we build a custom environment from scratch that we'll use for training. Then, we'll write a pipeline to prepare two datasets from the CIFAR10 image dataset, train the models using Keras, and then test and score the models.\n",
    "\n",
    "We deploy these models in [Deploy two models in one online deployment](). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the environment\n",
    "To build the environment, we start with the Azure minimal Ubuntu 18.04 image. We install the additional libraries `libpng` via apt and packages `tensorflow`, `pillow`, and `keras-preprocessing` via pip.\n",
    "## Inputs\n",
    "### Dockerfile\n",
    "```dockerfile\n",
    "FROM \"mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\"\n",
    "\n",
    "USER root\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y p7zip-full libpng-dev\n",
    "\n",
    "USER dockeruser\n",
    "\n",
    "COPY requirements.txt /tmp/requirements.txt\n",
    "\n",
    "RUN pip install -r /tmp/requirements.txt\n",
    "```\n",
    "### requirements.txt\n",
    "```\n",
    "tensorflow-gpu>=2.8, <2.9\n",
    "keras>= 2.8, <2.9\n",
    "pillow>= 9.1, <9.2\n",
    "keras-preprocessing>=1.1, <1.2\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build\n",
    "To build the environment, will call `az` with the environment file `environment.yaml` which contains the name of the environment `dualdeployment` as well as the directory in which the Dockerfile and `requirements.txt` file is located. \n",
    "\n",
    "```yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json\n",
    "name: dualdeployment\n",
    "build:\n",
    "  path: ./deploy\n",
    "```\n",
    "Build the environment with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml environment create --file environment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline\n",
    "Next, we assemble the code and YAML files to prepare the dataset, train the models, and test and score them. The complete pipeline job YAML file is reproduced at the end of this section. \n",
    "## Data Prep\n",
    "Our data comes from the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) which consists of 32x32 images divided into 10 classes. We use the [cifar10](https://pypi.org/project/cifar10/) Python wrapper to download the images and generate balanced test/train split datasets - one for cats and another for horses. We perform the data preprocessing in the `prep.py` file which is launched via the command job `prep-job` in `pipeline.yaml`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep.py (snippet)\n",
    "```python\n",
    "# Default parameters\n",
    "output_path = \"prep\"\n",
    "categories = {'cats' : 3, 'horses' : 7}\n",
    "train_size = .7\n",
    "\n",
    "# Generate dataset from CIFAR10 with test train split for one category\n",
    "def make_dataset(category_id, train_size=.7):\n",
    "    target = [(x[0], 1) for x in cifar10.data_batch_generator() if x[1] == category_id]\n",
    "    not_target = [x for x in cifar10.data_batch_generator() if x[1] != category_id]\n",
    "    ds = target + [(x[0], 0) for x in random.sample(not_target, len(target))] \n",
    "    random.shuffle(ds)\n",
    "    train_idxs = np.array(random.sample(range(len(ds)), int(len(ds)*train_size)))\n",
    "    ds_x = np.stack([x[0] for x in ds])\n",
    "    ds_y = np.array([x[1] for x in ds])\n",
    "    train = {'x': ds_x[train_idxs], 'y': ds_y[train_idxs]}\n",
    "    test = {'x': np.delete(ds_x, train_idxs, 0), 'y': np.delete(ds_y, train_idxs, 0)}\n",
    "    return {'train': train, 'test': test}\n",
    "\n",
    "datasets = {name : make_dataset(category_id, train_size) for name, category_id in categories.items()}\n",
    "output_path = Path(output_path)\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "for category, split in itertools.product(datasets.keys(), ('train','test')):\n",
    "    pth = output_path / category\n",
    "    try: \n",
    "        os.makedirs(pth)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    pickle.dump(datasets[category][split], open(pth / split, 'wb'))\n",
    "```\n",
    "### pipeline.yaml (snippet)\n",
    "The pipeline mounts storage for `prep.py` to output training and test data and passes the paths as arguments.\n",
    "```yaml\n",
    "  prep-job:\n",
    "    type: command\n",
    "    outputs:\n",
    "      train_data: \n",
    "        mode: upload\n",
    "      test_data: \n",
    "        mode: upload\n",
    "    code: src/prep\n",
    "    environment: azureml:dualdeployment:1\n",
    "    compute: azureml:dualdeploymentcompute\n",
    "    command: >-\n",
    "      python prep.py \n",
    "      --training_data ${{outputs.train_data}}\n",
    "      --test_data ${{outputs.test_data}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "In the `train.py` file, we build a simple CNN in Keras and save the model to the `model` output directory. \n",
    "### train.py (snippet)\n",
    "```python\n",
    "def make_model():\n",
    "    resize_rescale = k.Sequential([\n",
    "        k.layers.Resizing(32, 32),\n",
    "        k.layers.Rescaling(1./255)\n",
    "    ])\n",
    "    data_aug = k.Sequential([\n",
    "        k.layers.RandomFlip(),\n",
    "        k.layers.RandomContrast(.2)\n",
    "    ])\n",
    "    model = k.Sequential([\n",
    "        resize_rescale, \n",
    "        data_aug,\n",
    "        k.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        k.layers.Conv2D(32, 3, activation='relu', padding='same'), \n",
    "        k.layers.MaxPooling2D(2),\n",
    "        k.layers.BatchNormalization(),\n",
    "        k.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        k.layers.Conv2D(64, 3, activation='relu', padding='same'), \n",
    "        k.layers.MaxPooling2D(2),\n",
    "        k.layers.BatchNormalization(),\n",
    "        k.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        k.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        k.layers.MaxPooling2D(2),\n",
    "        k.layers.BatchNormalization(),\n",
    "        k.layers.Flatten(),\n",
    "        k.layers.Dense(128, activation = 'relu'),\n",
    "        k.layers.Dropout(0.1),\n",
    "        k.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(loss=k.losses.binary_crossentropy, \n",
    "                  optimizer=k.optimizers.Adam())\n",
    "    return model\n",
    "\n",
    "def run():\n",
    "    tf.keras.backend.clear_session()\n",
    "    tn_X = tf.convert_to_tensor(dataset['x'])\n",
    "    tn_Y = tf.convert_to_tensor(dataset['y'])\n",
    "    model = make_model()\n",
    "    model.fit(tn_X, tn_Y, batch_size=batch_size,epochs=epochs)\n",
    "    return model\n",
    "```\n",
    "### pipeline.yaml (snippet)\n",
    "The `train.py` file operates on one model at a time, so we include separate command jobs for the horse and cat models.\n",
    "```yaml\n",
    "  cats-train-job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      train_data: ${{parent.jobs.prep-job.outputs.train_data}}\n",
    "    outputs:\n",
    "      model: \n",
    "        mode: upload\n",
    "    code: src/prep\n",
    "    environment: azureml:dualdeployment:1\n",
    "    compute: azureml:dualdeploymentcompute\n",
    "    command: >-\n",
    "      python train.py \n",
    "      --train_data ${{inputs.train_data}}\n",
    "      --model ${{outputs.model}}\n",
    "  horses-train-job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      train_data: ${{parent.jobs.prep-job.outputs.train_data}}\n",
    "    outputs:\n",
    "      model: \n",
    "        mode: upload\n",
    "    code: src/prep\n",
    "    environment: azureml:dualdeployment:1\n",
    "    compute: azureml:dualdeploymentcompute\n",
    "    command: >-\n",
    "      python train.py \n",
    "      --train_data ${{inputs.train_data}}\n",
    "      --model ${{outputs.model}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "To build and run the pipeline, run the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create --file pipeline.yaml"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50db1dce3900200bed9bfd600df88d2ae354b85acd9708fa0a0be5ff9bc29e40"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
